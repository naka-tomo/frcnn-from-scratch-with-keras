{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"frcnn.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMElCtFxL+EVX7J2w1+wq0w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vfzMZRXB096o","colab_type":"code","outputId":"09ddf82e-18a9-405c-b131-8ca7b168c685","executionInfo":{"status":"ok","timestamp":1591949534745,"user_tz":-540,"elapsed":6123,"user":{"displayName":"中村友昭","photoUrl":"","userId":"08566676536456149802"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["!git clone https://github.com/naka-tomo/frcnn-from-scratch-with-keras.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'frcnn-from-scratch-with-keras'...\n","remote: Enumerating objects: 15, done.\u001b[K\n","remote: Counting objects: 100% (15/15), done.\u001b[K\n","remote: Compressing objects: 100% (15/15), done.\u001b[K\n","remote: Total 1199 (delta 5), reused 6 (delta 0), pack-reused 1184\u001b[K\n","Receiving objects: 100% (1199/1199), 15.95 MiB | 7.55 MiB/s, done.\n","Resolving deltas: 100% (788/788), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sL5cdylO1d9L","colab_type":"code","outputId":"e54d35d7-daa6-46ba-90e7-8b61cc30ca6e","executionInfo":{"status":"ok","timestamp":1591949536936,"user_tz":-540,"elapsed":1067,"user":{"displayName":"中村友昭","photoUrl":"","userId":"08566676536456149802"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd frcnn-from-scratch-with-keras/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/frcnn-from-scratch-with-keras\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cmL7piwsIusw","colab_type":"code","outputId":"cd8df671-18a6-4414-b5cb-b3e0dc88f8c5","executionInfo":{"status":"ok","timestamp":1591949560464,"user_tz":-540,"elapsed":22690,"user":{"displayName":"中村友昭","photoUrl":"","userId":"08566676536456149802"}},"colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["# 学習済みモデルをダウンロード\n","!gdown https://drive.google.com/uc?id=1BL_2ZgTf55vH2q1jvVz0hkhlWYgj-coa\n","!gdown https://drive.google.com/uc?id=1IgxPP0aI5pxyPHVSM2ZJjN1p9dtE4_64\n","!mkdir -p models/vgg/\n","!mv voc.hdf5 models/vgg/"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1BL_2ZgTf55vH2q1jvVz0hkhlWYgj-coa\n","To: /content/frcnn-from-scratch-with-keras/config.pickle\n","100% 1.10k/1.10k [00:00<00:00, 435kB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1IgxPP0aI5pxyPHVSM2ZJjN1p9dtE4_64\n","To: /content/frcnn-from-scratch-with-keras/voc.hdf5\n","548MB [00:09, 59.5MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b8_2xGV31nfP","colab_type":"code","outputId":"1f55ae3f-0f5f-40e8-c92c-4285c1e4354e","executionInfo":{"status":"ok","timestamp":1591949583744,"user_tz":-540,"elapsed":21414,"user":{"displayName":"中村友昭","photoUrl":"","userId":"08566676536456149802"}},"colab":{"base_uri":"https://localhost:8080/","height":222}},"source":["%cd frcnn-from-scratch-with-keras/\n","from __future__ import division\n","import os\n","import cv2\n","import numpy as np\n","import sys\n","import pickle\n","from optparse import OptionParser\n","import time\n","import glob\n","from keras_frcnn import config\n","from keras import backend as K\n","from keras.layers import Input\n","from keras.models import Model\n","from keras_frcnn import roi_helpers\n","from keras_frcnn.pascal_voc import pascal_voc_util\n","from keras_frcnn.pascal_voc_parser import get_data\n","from keras_frcnn import data_generators\n","\n","from utils import get_bbox\n","\n","sys.setrecursionlimit(40000)\n","\n","\"\"\"\n","parser = OptionParser()\n","\n","parser.add_option(\"-p\", \"--path\", dest=\"test_path\", help=\"Path to test data.\")\n","parser.add_option(\"-n\", \"--num_rois\", type=\"int\", dest=\"num_rois\",\n","\t\t\t\thelp=\"Number of ROIs per iteration. Higher means more memory use.\", default=32)\n","parser.add_option(\"--config_filename\", dest=\"config_filename\", help=\n","\t\t\t\t\"Location to read the metadata related to the training (generated when training).\",\n","\t\t\t\tdefault=\"config.pickle\")\n","parser.add_option(\"--network\", dest=\"network\", help=\"Base network to use. Supports vgg or resnet50.\", default='resnet50')\n","parser.add_option(\"--write\", dest=\"write\", help=\"to write out the image with detections or not.\", action='store_true')\n","parser.add_option(\"--load\", dest=\"load\", help=\"specify model path.\", default=None)\n","(options, args) = parser.parse_args()\n","\"\"\"\n","\n","# オプション\n","class options:\n","  test_path = \"images\"\n","  num_rois = 32\n","  config_filename = \"config.pickle\"\n","  network = \"vgg\"\n","  write = True\n","  load = None\n","\n","# K.image_dim_ordering()がなくなったため，とりあえず対処\n","def image_dim_ordering():\n","  return \"tf\"\n","\n","if not options.test_path:   # if filename is not given\n","\tparser.error('Error: path to test data must be specified. Pass --path to command line')\n","\n","config_output_filename = options.config_filename\n","\n","with open(config_output_filename, 'rb') as f_in:\n","\tC = pickle.load(f_in)\n","\n","# we will use resnet. may change to vgg\n","if options.network == 'vgg':\n","\tC.network = 'vgg16'\n","\tfrom keras_frcnn import vgg as nn\n","elif options.network == 'resnet50':\n","\tfrom keras_frcnn import resnet as nn\n","\tC.network = 'resnet50'\n","elif options.network == 'vgg19':\n","\tfrom keras_frcnn import vgg19 as nn\n","\tC.network = 'vgg19'\n","elif options.network == 'mobilenetv1':\n","\tfrom keras_frcnn import mobilenetv1 as nn\n","\tC.network = 'mobilenetv1'\n","elif options.network == 'mobilenetv1_05':\n","\tfrom keras_frcnn import mobilenetv1_05 as nn\n","\tC.network = 'mobilenetv1_05'\n","elif options.network == 'mobilenetv1_25':\n","\tfrom keras_frcnn import mobilenetv1_25 as nn\n","\tC.network = 'mobilenetv1_25'\n","elif options.network == 'mobilenetv2':\n","\tfrom keras_frcnn import mobilenetv2 as nn\n","\tC.network = 'mobilenetv2'\n","else:\n","\tprint('Not a valid model')\n","\traise ValueError\n","\n","# turn off any data augmentation at test time\n","C.use_horizontal_flips = False\n","C.use_vertical_flips = False\n","C.rot_90 = False\n","\n","img_path = options.test_path\n","# Method to transform the coordinates of the bounding box to its original size\n","def get_real_coordinates(ratio, x1, y1, x2, y2):\n","\treal_x1 = int(round(x1 // ratio))\n","\treal_y1 = int(round(y1 // ratio))\n","\treal_x2 = int(round(x2 // ratio))\n","\treal_y2 = int(round(y2 // ratio))\n","\n","\treturn (real_x1, real_y1, real_x2 ,real_y2)\n","def format_img_size(img, C):\n","\t\"\"\" formats the image size based on config \"\"\"\n","\timg_min_side = float(C.im_size)\n","\t(height,width,_) = img.shape\n","\t\t\n","\tif width <= height:\n","\t\tratio = img_min_side/width\n","\t\tnew_height = int(ratio * height)\n","\t\tnew_width = int(img_min_side)\n","\telse:\n","\t\tratio = img_min_side/height\n","\t\tnew_width = int(ratio * width)\n","\t\tnew_height = int(img_min_side)\n","\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n","\treturn img, ratio\t\n","\n","def format_img_channels(img, C):\n","\t\"\"\" formats the image channels based on config \"\"\"\n","\timg = img[:, :, (2, 1, 0)]\n","\timg = img.astype(np.float32)\n","\timg[:, :, 0] -= C.img_channel_mean[0]\n","\timg[:, :, 1] -= C.img_channel_mean[1]\n","\timg[:, :, 2] -= C.img_channel_mean[2]\n","\timg /= C.img_scaling_factor\n","\timg = np.transpose(img, (2, 0, 1))\n","\timg = np.expand_dims(img, axis=0)\n","\treturn img\n","\n","def format_img(img, C):\n","\t\"\"\" formats an image for model prediction based on config \"\"\"\n","\timg, ratio = format_img_size(img, C)\n","\timg = format_img_channels(img, C)\n","\treturn img, ratio\n","\n","class_mapping = C.class_mapping\n","\n","if 'bg' not in class_mapping:\n","\tclass_mapping['bg'] = len(class_mapping)\n","\n","class_mapping = {v: k for k, v in class_mapping.items()}\n","print(class_mapping)\n","class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}\n","C.num_rois = int(options.num_rois)\n","\n","if C.network == 'resnet50':\n","\tnum_features = 1024\n","else:\n","\t# may need to fix this up with your backbone..!\n","\tprint(\"backbone is not resnet50. number of features chosen is 512\")\n","\tnum_features = 512\n","\n","\n","if image_dim_ordering() == 'th':\n","\tinput_shape_img = (3, None, None)\n","\tinput_shape_features = (num_features, None, None)\n","else:\n","\tinput_shape_img = (None, None, 3)\n","\tinput_shape_features = (None, None, num_features)\n","\n","\n","img_input = Input(shape=input_shape_img)\n","roi_input = Input(shape=(C.num_rois, 4))\n","feature_map_input = Input(shape=input_shape_features)\n","\n","# define the base network (resnet here, can be VGG, Inception, etc)\n","shared_layers = nn.nn_base(img_input, trainable=True)\n","\n","# define the RPN, built on the base layers\n","num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n","rpn_layers = nn.rpn(shared_layers, num_anchors)\n","\n","classifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n","\n","model_rpn = Model(img_input, rpn_layers)\n","#model_classifier_only = Model([feature_map_input, roi_input], classifier)\n","model_classifier = Model([feature_map_input, roi_input], classifier)\n","\n","# model loading\n","if options.load == None:\n","  print('Loading weights from {}'.format(C.model_path))\n","  model_rpn.load_weights(C.model_path, by_name=True)\n","  model_classifier.load_weights(C.model_path, by_name=True)\n","else:\n","  print('Loading weights from {}'.format(options.load))\n","  model_rpn.load_weights(options.load, by_name=True)\n","  model_classifier.load_weights(options.load, by_name=True)\n","\n","model_rpn.compile(optimizer='sgd', loss='mse')\n","model_classifier.compile(optimizer='sgd', loss='mse')\n","\n","all_imgs = []\n","\n","classes = {}\n","\n","bbox_threshold = 0.8\n","\n","visualise = True\n","\n","# define pascal\n","#pascal = pascal_voc_util(options.test_path)\n","\n","# define dataloader\n","\"\"\"\n","all_imgs, classes_count, class_mapping = get_data(options.test_path)\n","val_imgs = [s for s in all_imgs if s['imageset'] == 'test']\n","if len(val_imgs) == 0:\n","    print(\"val images not found. using trainval images for testing.\")\n","    val_imgs = [s for s in all_imgs if s['imageset'] == 'trainval'] # for test purpose\n","    \n","print('Num val samples {}'.format(len(val_imgs)))\n","data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,image_dim_ordering(), mode='val')\n","\"\"\"\n","#img_pathes = [x[\"filepath\"] for x in val_imgs]\n","img_pathes = glob.glob( os.path.join( options.test_path, \"*\" ) )\n","print(img_pathes)\n","\n","# define detections\n","#all_boxes = [[[] for _ in range(len(val_imgs))] for _ in range(20)]\n","\n","for idx, img_name in enumerate(sorted(img_pathes)):\n","\tif not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n","\t\tcontinue\n","\tprint(img_name)\n","\tst = time.time()\n","\t#filepath = os.path.join(img_path,img_name)\n","  \n","\n","\timg = cv2.imread(img_name)\n","\n","\tX, ratio = format_img(img, C)\n","\timg_scaled = (np.transpose(X[0,:,:,:],(1,2,0)) + 127.5).astype('uint8')\n","\n","\tif image_dim_ordering() == 'tf':\n","\t\tX = np.transpose(X, (0, 2, 3, 1))\n","\n","\t# get the feature maps and output from the RPN\n","\t[Y1, Y2, F] = model_rpn.predict(X)\n","\t\n","    # infer roi\n","\tR = roi_helpers.rpn_to_roi(Y1, Y2, C, image_dim_ordering(), overlap_thresh=0.7)\n","    # get bbox\n","#\tall_dets, bboxes, probs = get_bbox(R, C, model_classifier, class_mapping, F, ratio, bbox_threshold=0.5)\n","    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n","\tR[:, 2] -= R[:, 0]\n","\tR[:, 3] -= R[:, 1]\n","\n","\t# apply the spatial pyramid pooling to the proposed regions\n","\tbboxes = {}\n","\tprobs = {}\n","\n","\tfor jk in range(R.shape[0]//C.num_rois + 1):\n","\t\tROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n","\t\tif ROIs.shape[1] == 0:\n","\t\t\tbreak\n","\n","\t\tif jk == R.shape[0]//C.num_rois:\n","\t\t\t#pad R\n","\t\t\tcurr_shape = ROIs.shape\n","\t\t\ttarget_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n","\t\t\tROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n","\t\t\tROIs_padded[:, :curr_shape[1], :] = ROIs\n","\t\t\tROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n","\t\t\tROIs = ROIs_padded\n","\n","\t\t[P_cls, P_regr] = model_classifier.predict([F, ROIs])\n","\n","\t\tfor ii in range(P_cls.shape[1]):\n","\n","\t\t\tif np.max(P_cls[0, ii, :]) < bbox_threshold: #or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n","#\t\t\t\tprint(\"no boxes detected\")\n","\t\t\t\tcontinue\n","\t\t\tcls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n","\n","\t\t\tif cls_name not in bboxes:\n","\t\t\t\tbboxes[cls_name] = []\n","\t\t\t\tprobs[cls_name] = []\n","\n","\t\t\t(x, y, w, h) = ROIs[0, ii, :]\n","\n","\t\t\tcls_num = np.argmax(P_cls[0, ii, :])\n","\t\t\ttry:\n","\t\t\t\t(tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n","\t\t\t\ttx /= C.classifier_regr_std[0]\n","\t\t\t\tty /= C.classifier_regr_std[1]\n","\t\t\t\ttw /= C.classifier_regr_std[2]\n","\t\t\t\tth /= C.classifier_regr_std[3]\n","\t\t\t\tx, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n","\t\t\texcept:\n","\t\t\t\tpass\n","\t\t\tbboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n","\t\t\tprobs[cls_name].append(np.max(P_cls[0, ii, :]))\n","\tall_dets = []\n","\tfor key in bboxes:\n","\t\tbbox = np.array(bboxes[key])\n","\n","\t\tnew_boxes, new_probs = roi_helpers.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n","\t\tfor jk in range(new_boxes.shape[0]):\n","\t\t\t(x1, y1, x2, y2) = new_boxes[jk,:]\n","\n","\t\t\t(real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n","\n","#\t\t\tcv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),2)\n","\t\t\ttextLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n","\t\t\tall_dets.append((key,100*new_probs[jk]))\n","\t\t\t(retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n","#\t\t\ttextOrg = (real_x1, real_y1-0)            \n","\n","\tprint('Elapsed time = {}'.format(time.time() - st))\n","\tprint(\"det:\", all_dets)\n","\tprint(\"boxes:\", bboxes)\n","    # enable if you want to show pics\n","\t#cv2.imshow('img', img)\n","\t#cv2.waitKey(0)\n","\tif options.write:\n","           import os\n","           if not os.path.isdir(\"results\"):\n","              os.mkdir(\"results\")\n","           cv2.imwrite('./results/{}.png'.format(idx),img)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'frcnn-from-scratch-with-keras/'\n","/content/frcnn-from-scratch-with-keras\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["{0: 'dog', 1: 'cat', 2: 'car', 3: 'person', 4: 'chair', 5: 'bottle', 6: 'diningtable', 7: 'pottedplant', 8: 'bird', 9: 'horse', 10: 'motorbike', 11: 'bus', 12: 'tvmonitor', 13: 'sofa', 14: 'boat', 15: 'cow', 16: 'aeroplane', 17: 'train', 18: 'sheep', 19: 'bicycle', 20: 'bg'}\n","backbone is not resnet50. number of features chosen is 512\n","Loading weights from models/vgg/voc.hdf5\n","['images/85.png', 'images/__init__.py']\n","images/85.png\n","Elapsed time = 8.79607343673706\n","det: [('person', 99.33544397354126), ('person', 92.44018793106079), ('person', 86.44517064094543), ('bg', 99.63776469230652), ('bg', 99.27207827568054), ('bg', 99.26531314849854), ('bg', 99.25766587257385), ('bg', 99.18525815010071), ('bg', 99.08103942871094), ('bg', 98.97822141647339), ('bg', 98.92052412033081), ('bg', 98.90788197517395), ('bg', 98.89910221099854), ('bg', 98.87822270393372), ('bg', 98.81187081336975), ('bg', 98.73831868171692), ('bg', 98.6130952835083), ('bg', 98.47168922424316), ('bg', 98.41533303260803), ('bg', 98.35687279701233), ('bg', 98.30416440963745), ('bg', 98.12109470367432), ('bg', 98.01756739616394), ('bg', 97.98250794410706), ('bg', 97.84708619117737), ('bg', 97.84351587295532), ('bg', 97.57693409919739), ('bg', 97.12997674942017), ('bg', 96.75729870796204), ('bg', 96.5082049369812), ('bg', 96.384596824646), ('bg', 95.99618911743164), ('bg', 95.68188786506653), ('bg', 95.40921449661255), ('bg', 95.14595866203308), ('bg', 95.12029886245728), ('bg', 94.82781887054443), ('bg', 94.69427466392517), ('bg', 94.6809470653534), ('bg', 94.64117884635925), ('bg', 93.18033456802368), ('bg', 93.12000870704651), ('bg', 92.80208945274353), ('bg', 92.79303550720215), ('bg', 92.79209971427917), ('bg', 92.75520443916321), ('bg', 92.5601601600647), ('bg', 92.03494787216187), ('bg', 91.60345196723938), ('bg', 91.2567138671875), ('bg', 90.29345512390137), ('bg', 89.03631567955017), ('bg', 89.0110433101654), ('bg', 88.91378045082092), ('bg', 87.9925787448883), ('bg', 87.68110275268555), ('bg', 87.64292001724243), ('bg', 87.03479766845703), ('bg', 86.53022050857544), ('bg', 86.35003566741943), ('bg', 85.63960194587708), ('bg', 84.66778993606567), ('bg', 84.3819260597229), ('bg', 84.16905403137207), ('bg', 84.13079380989075), ('bg', 84.03165936470032), ('bg', 83.00502896308899), ('bg', 81.39707446098328), ('bg', 81.14388585090637), ('bg', 80.73576092720032)]\n","boxes: {'person': [[224, 64, 528, 544], [144, 96, 560, 592], [192, 96, 512, 560], [16, 64, 544, 560], [208, 80, 512, 544], [160, 96, 480, 560], [208, 80, 560, 544], [176, 64, 464, 560], [128, 128, 560, 496], [32, 160, 560, 512], [0, 176, 80, 304], [96, 112, 544, 480], [0, 176, 64, 288], [240, 80, 512, 512], [224, 128, 528, 480], [0, 176, 48, 320], [208, 128, 512, 496]], 'bg': [[80, 0, 768, 288], [304, 272, 784, 576], [128, 0, 752, 192], [160, 192, 736, 576], [176, 304, 752, 576], [368, 64, 784, 528], [352, 0, 784, 240], [336, 352, 784, 576], [160, 128, 640, 416], [112, 48, 656, 336], [368, 160, 624, 576], [0, 144, 192, 416], [176, 416, 752, 576], [400, 432, 784, 576], [528, 336, 784, 576], [304, 128, 784, 384], [400, 192, 784, 448], [128, 448, 704, 576], [48, 416, 624, 576], [96, 304, 144, 384], [384, 160, 784, 400], [384, 32, 656, 512], [96, 320, 160, 400], [240, 96, 784, 352], [656, 240, 720, 304], [192, 176, 704, 496], [304, 208, 784, 512], [144, 0, 448, 272], [0, 304, 128, 576], [432, 16, 704, 464], [400, 256, 784, 512], [416, 144, 688, 416], [288, 160, 784, 448], [96, 384, 704, 576], [0, 192, 352, 464], [16, 208, 80, 320], [112, 320, 176, 400], [288, 0, 640, 272], [288, 176, 560, 576], [192, 80, 688, 368], [528, 0, 784, 128], [688, 240, 736, 304], [336, 304, 624, 576], [80, 320, 128, 400], [624, 336, 784, 576], [656, 0, 784, 256], [720, 496, 768, 576], [528, 448, 784, 576], [0, 256, 48, 352], [176, 0, 704, 256], [256, 0, 592, 320], [352, 0, 640, 432], [656, 48, 784, 544], [368, 0, 656, 240], [688, 256, 736, 320], [128, 192, 256, 448], [400, 240, 688, 576], [752, 496, 784, 576], [0, 0, 256, 128], [0, 448, 96, 576], [0, 464, 48, 560], [0, 0, 128, 304], [0, 176, 256, 432], [224, 320, 496, 576], [560, 448, 688, 576], [720, 448, 784, 576], [752, 0, 784, 96], [656, 288, 784, 576], [320, 240, 592, 576], [128, 432, 256, 576], [736, 528, 784, 576], [64, 448, 192, 576], [0, 464, 80, 560], [656, 448, 784, 576], [288, 336, 560, 576], [736, 0, 784, 64], [80, 480, 176, 576], [0, 448, 64, 576], [608, 272, 688, 352], [0, 496, 48, 576], [0, 0, 48, 48], [432, 336, 704, 576], [64, 336, 592, 576], [144, 448, 272, 576], [608, 240, 656, 304], [752, 256, 784, 352], [0, 176, 48, 416], [656, 0, 784, 128], [752, 32, 784, 128], [752, 240, 784, 320], [704, 480, 752, 576], [752, 480, 784, 560], [0, 528, 48, 576], [736, 496, 784, 576], [496, 432, 752, 576], [0, 0, 240, 240], [592, 448, 720, 576], [672, 0, 784, 80], [288, 528, 384, 576], [752, 368, 784, 448], [512, 240, 784, 496], [688, 272, 736, 336], [96, 336, 160, 432], [128, 320, 192, 384], [448, 48, 736, 544], [768, 496, 784, 560], [80, 176, 352, 448], [528, 448, 656, 576], [752, 384, 784, 464], [480, 224, 528, 320], [432, 224, 688, 512], [720, 0, 784, 128], [16, 480, 96, 576], [704, 528, 784, 576], [768, 192, 784, 272], [0, 0, 512, 144], [544, 432, 672, 576], [752, 80, 784, 176], [16, 256, 80, 352], [0, 288, 48, 384], [752, 288, 784, 368], [592, 240, 640, 320], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576], [704, 528, 784, 576]]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qng0aJE8-eRX","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}